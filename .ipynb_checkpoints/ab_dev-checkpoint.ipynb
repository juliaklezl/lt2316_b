{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LT2316 H20 Assignment B - image autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will define an image autoencoder model by changing/adding a limited amount of code to this notebook.  We will mark off what you will change.  \n",
    "\n",
    "An autoencoder is a machine learning system/network that attempts to reconstruct the input (or a proxy for the input such as its context, as in neural language modeling) after compressing it to a smaller \"bottleneck\" representation. We can then extract the compressed representation for the input by running the input through the part of the trained model that ends in the compressing hidden layer.  For images, the input and output of the full model usually have the same shape, and the loss is the pixel-by-pixel colour channel error.  The embeddings of the images are extracted from some middle layer as a vector of much lower dimensionality.\n",
    "\n",
    "We can then examine how \"good\" the embeddings are not only by the training loss but also by other techniques, such as clustering the embeddings. We will just test on the training data for simplicity (this is not always wrong if our goal is merely to get a generalized/compressed representation of a fixed amount of data).\n",
    "\n",
    "Below we will mark off what you need to change and what you can change in markdown and code comments. The rest should remain untouched when you submit.  You are recommended to develop in a copy of the notebook you will submit and then port over your changes to the \"final\" notebook. That way, you can modify our code to test your code in your private notebook.\n",
    "\n",
    "You will submit a saved notebook directly to Canvas.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random \n",
    "from PIL import Image\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "# You may add imports you feel you need for the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=18.43s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "coco = COCO(annotation_file=\"/scratch/lt2316-h18-resources/coco/annotations/instances_train2017.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_device = \"cuda:1\" # you can change the device to another GPU or the cpu for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(meta, datadir=\"/scratch/lt2316-h18-resources/coco/train2017\"):\n",
    "    return [(x['file_name'], Image.open(\"{}/{}\".format(datadir, x['file_name'])).resize((100,100)).convert('RGB')) for x in meta]\n",
    "\n",
    "def cat_img_load(category, coco, trainsize):\n",
    "    catids = coco.getCatIds(catNms=category)\n",
    "    imgids = coco.getImgIds(catIds=catids)\n",
    "    \n",
    "    random.shuffle(imgids)\n",
    "    imgids = imgids[:trainsize]\n",
    "    imgmeta = coco.loadImgs(ids=imgids)\n",
    "    imgdata = get_data(imgmeta)\n",
    "    \n",
    "    imgdf = pd.DataFrame()\n",
    "    imgnames = [x[0] for x in imgdata]\n",
    "    imgarrays = [x[1] for x in imgdata]\n",
    "    imgdf['imgs'] = imgarrays\n",
    "    imgdf['filename'] = imgnames\n",
    "    imgdf['class'] = category\n",
    "    \n",
    "    return imgdf\n",
    "\n",
    "def get_tensors(*imgdfs):\n",
    "    bigdf = pd.concat(imgdfs)\n",
    "    print(len(bigdf))\n",
    "    X = np.array([np.array(x) for x in bigdf['imgs']])\n",
    "    print(X.shape)\n",
    "    y = bigdf['class']\n",
    "    filenames = bigdf['filename']\n",
    "    X_scaled = StandardScaler().fit_transform(X.reshape(len(X),30000)).reshape(len(X), 100, 100, 3)\n",
    "    X_tensor = torch.Tensor(X_scaled).to(my_device)\n",
    "    return X_tensor, y, filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We may change these MS COCO categories when testing as well as the number of retrieved items.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "airplanedf = cat_img_load(\"airplane\", coco, 1000)\n",
    "skateboarddf = cat_img_load(\"skateboard\", coco, 1000)\n",
    "mousedf = cat_img_load(\"mouse\", coco, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000, 1000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(airplanedf), len(skateboarddf), len(mousedf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "(3000, 100, 100, 3)\n"
     ]
    }
   ],
   "source": [
    "X_tensor, y, filenames = get_tensors(airplanedf, skateboarddf, mousedf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching and shuffling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**There should be no reason to edit this.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batcher:\n",
    "    def __init__(self, X, device, batch_size=50, max_iter=None):\n",
    "        self.X = X\n",
    "        self.device = device\n",
    "        self.batch_size=batch_size\n",
    "        self.max_iter = max_iter\n",
    "        self.curr_iter = 0\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.curr_iter == self.max_iter:\n",
    "            raise StopIteration\n",
    "        permutation = torch.randperm(self.X.size()[0], device=self.device)\n",
    "        permX = self.X[permutation]\n",
    "        splitX = torch.split(permX, self.batch_size)\n",
    "        \n",
    "        self.curr_iter += 1\n",
    "        return splitX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here is where you will make the main changes.**\n",
    "\n",
    "You will get **10 points** for making changes that run and produce representations that are *emb_size* in width when we extract the embeddings by calling *emb* on the training data and represent a good-faith attempt at building a simple autoencoder.  \n",
    "\n",
    "We will give **1 to 3 points** for model design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageAutoencoder(nn.Module):\n",
    "    # You may ADD hyperparameters here (and make requisite adaptations in the training loop below).\n",
    "    # Document them under \"Your analysis\"\n",
    "    def __init__(self, emb_size, height, width):\n",
    "        super(ImageAutoencoder, self).__init__()\n",
    "        self.emb_size = emb_size\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "        \n",
    "        #input_size = height*width*3\n",
    "        input_size = width * 3\n",
    "        #input_size = 3\n",
    "        \n",
    "        # Define your network structure here using PyTorch.\n",
    "        self.enc1 = nn.Linear(in_features=input_size, out_features=input_size//2)\n",
    "        self.enc2 = nn.Linear(in_features=input_size//2, out_features=input_size//4)\n",
    "        self.enc3 = nn.Linear(in_features=input_size//4, out_features=emb_size)\n",
    "        self.dec1 = nn.Linear(in_features=emb_size, out_features=input_size//4)\n",
    "        self.dec2 = nn.Linear(in_features=input_size//4, out_features=input_size//2)\n",
    "        self.dec3 = nn.Linear(in_features=input_size//2, out_features=input_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, batch):\n",
    "        # Apply the model to the batch here.  Watch out for the shapes!\n",
    "        batch = batch.flatten(start_dim=2)\n",
    "        \n",
    "        en1 = self.enc1(batch)\n",
    "        en1 = torch.relu(en1)\n",
    "        en2 = self.enc2(en1)\n",
    "        en2 = torch.relu(en2)\n",
    "        en3 = self.enc3(en2)\n",
    "        en3 = torch.relu(en3)\n",
    "        de1 = self.dec1(en3)\n",
    "        de1 = torch.relu(de1)\n",
    "        de2 = self.dec2(de1)\n",
    "        de2 = torch.relu(de2)\n",
    "        de3 = self.dec3(de2)\n",
    "        de3 = torch.relu(de3)\n",
    "        \n",
    "        out = de3.view(-1, self.height, self.width, 3)\n",
    "    \n",
    "        return out\n",
    "        \n",
    "    def emb(self, batch):\n",
    "        # This should return the inner representation of the images, including for arbitrary unseen images\n",
    "        # of the correct shape.\n",
    "        batch = batch.flatten(start_dim=2)\n",
    "        en1 = self.enc1(batch)\n",
    "        en1 = torch.relu(en1)\n",
    "        en2 = self.enc2(en1)\n",
    "        en2 = torch.relu(en2)\n",
    "        en3 = self.enc3(en2)\n",
    "        en3 = torch.relu(en3)\n",
    "        \n",
    "        out = en3.view(-1, self.height, self.emb_size)\n",
    "        out = out.flatten(start_dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**You may make limited changes here.**\n",
    "\n",
    "You can adapt *train()* slightly to handle any hyperparameters you added to *ImageAutoencoder*.  We may test by changing the values of the hyperparameters when we grade the assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, batch_size, epochs, device, model=None):\n",
    "    b = Batcher(X, device, batch_size=batch_size, max_iter=epochs)\n",
    "    if not model:\n",
    "         #We may change the embedding size by hand here. \n",
    "        m = ImageAutoencoder(400, X[0].size()[0], X[0].size()[1]).to(device)\n",
    "    else:\n",
    "        m = model\n",
    "    loss = nn.MSELoss()\n",
    "    optimizer = optim.Adam(m.parameters(), lr=0.005)\n",
    "    epoch = 0\n",
    "    for split in b:\n",
    "        tot_loss = 0\n",
    "        for batch in split:\n",
    "            optimizer.zero_grad()\n",
    "            o = m(batch)\n",
    "            l = loss(o, batch)\n",
    "            tot_loss += l\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "        print(\"Total loss in epoch {} is {}.\".format(epoch, tot_loss))\n",
    "        epoch += 1\n",
    "    return m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model and checking the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're leaving the results of running our own simple model here just so you know what it might look like, but there's no guarantee or requirement that the performance of your model will be similar.  It's reasonably likely that it might even be better...whatever we mean by better. But it will very likely be different, especially as there is some randomness involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total loss in epoch 0 is 77.28620910644531.\n",
      "Total loss in epoch 1 is 69.22488403320312.\n",
      "Total loss in epoch 2 is 67.10160827636719.\n",
      "Total loss in epoch 3 is 65.97740173339844.\n",
      "Total loss in epoch 4 is 65.85813903808594.\n",
      "Total loss in epoch 5 is 65.11811065673828.\n",
      "Total loss in epoch 6 is 64.63735961914062.\n",
      "Total loss in epoch 7 is 64.27767181396484.\n",
      "Total loss in epoch 8 is 64.11898803710938.\n",
      "Total loss in epoch 9 is 63.89195251464844.\n",
      "Total loss in epoch 10 is 63.77454376220703.\n",
      "Total loss in epoch 11 is 63.827762603759766.\n",
      "Total loss in epoch 12 is 63.478363037109375.\n",
      "Total loss in epoch 13 is 63.40165328979492.\n",
      "Total loss in epoch 14 is 63.106842041015625.\n",
      "Total loss in epoch 15 is 63.09342575073242.\n",
      "Total loss in epoch 16 is 63.019775390625.\n",
      "Total loss in epoch 17 is 62.75934600830078.\n",
      "Total loss in epoch 18 is 62.981109619140625.\n",
      "Total loss in epoch 19 is 62.73892593383789.\n",
      "Total loss in epoch 20 is 62.74154281616211.\n",
      "Total loss in epoch 21 is 62.59525680541992.\n",
      "Total loss in epoch 22 is 62.52356719970703.\n",
      "Total loss in epoch 23 is 62.39510726928711.\n",
      "Total loss in epoch 24 is 62.58788299560547.\n",
      "Total loss in epoch 25 is 62.34929656982422.\n",
      "Total loss in epoch 26 is 62.186893463134766.\n",
      "Total loss in epoch 27 is 62.15620803833008.\n",
      "Total loss in epoch 28 is 62.557186126708984.\n",
      "Total loss in epoch 29 is 62.17665481567383.\n",
      "Total loss in epoch 30 is 61.96318435668945.\n",
      "Total loss in epoch 31 is 62.034183502197266.\n",
      "Total loss in epoch 32 is 61.96144485473633.\n",
      "Total loss in epoch 33 is 61.8937873840332.\n",
      "Total loss in epoch 34 is 62.00419235229492.\n",
      "Total loss in epoch 35 is 61.7626953125.\n",
      "Total loss in epoch 36 is 61.824119567871094.\n",
      "Total loss in epoch 37 is 61.7161865234375.\n",
      "Total loss in epoch 38 is 61.84543991088867.\n",
      "Total loss in epoch 39 is 61.73898696899414.\n",
      "Total loss in epoch 40 is 61.85966873168945.\n",
      "Total loss in epoch 41 is 61.73408126831055.\n",
      "Total loss in epoch 42 is 61.71663284301758.\n",
      "Total loss in epoch 43 is 61.90034484863281.\n",
      "Total loss in epoch 44 is 61.64471435546875.\n",
      "Total loss in epoch 45 is 61.78434371948242.\n",
      "Total loss in epoch 46 is 61.69982147216797.\n",
      "Total loss in epoch 47 is 61.82359313964844.\n",
      "Total loss in epoch 48 is 61.63667297363281.\n",
      "Total loss in epoch 49 is 61.526493072509766.\n",
      "Total loss in epoch 50 is 61.660133361816406.\n",
      "Total loss in epoch 51 is 61.53243637084961.\n",
      "Total loss in epoch 52 is 61.533058166503906.\n",
      "Total loss in epoch 53 is 61.577415466308594.\n",
      "Total loss in epoch 54 is 61.53300094604492.\n",
      "Total loss in epoch 55 is 61.71920394897461.\n",
      "Total loss in epoch 56 is 61.59475326538086.\n",
      "Total loss in epoch 57 is 61.50276184082031.\n",
      "Total loss in epoch 58 is 61.63001251220703.\n",
      "Total loss in epoch 59 is 61.44659423828125.\n",
      "Total loss in epoch 60 is 61.627235412597656.\n",
      "Total loss in epoch 61 is 61.54130935668945.\n",
      "Total loss in epoch 62 is 61.46065902709961.\n",
      "Total loss in epoch 63 is 61.43024826049805.\n",
      "Total loss in epoch 64 is 61.49652862548828.\n",
      "Total loss in epoch 65 is 61.547019958496094.\n",
      "Total loss in epoch 66 is 61.50263977050781.\n",
      "Total loss in epoch 67 is 61.40862274169922.\n",
      "Total loss in epoch 68 is 61.65456008911133.\n",
      "Total loss in epoch 69 is 61.574378967285156.\n",
      "Total loss in epoch 70 is 61.44189453125.\n",
      "Total loss in epoch 71 is 61.59954071044922.\n",
      "Total loss in epoch 72 is 61.40032958984375.\n",
      "Total loss in epoch 73 is 61.421443939208984.\n",
      "Total loss in epoch 74 is 61.35475540161133.\n",
      "Total loss in epoch 75 is 61.37531661987305.\n",
      "Total loss in epoch 76 is 61.53044891357422.\n",
      "Total loss in epoch 77 is 61.547828674316406.\n",
      "Total loss in epoch 78 is 61.64949035644531.\n",
      "Total loss in epoch 79 is 61.37376403808594.\n",
      "Total loss in epoch 80 is 61.538082122802734.\n",
      "Total loss in epoch 81 is 61.37919998168945.\n",
      "Total loss in epoch 82 is 61.4504508972168.\n",
      "Total loss in epoch 83 is 61.51127243041992.\n",
      "Total loss in epoch 84 is 61.506813049316406.\n",
      "Total loss in epoch 85 is 61.3743782043457.\n",
      "Total loss in epoch 86 is 61.47997283935547.\n",
      "Total loss in epoch 87 is 61.40852737426758.\n",
      "Total loss in epoch 88 is 61.395626068115234.\n",
      "Total loss in epoch 89 is 61.50107955932617.\n",
      "Total loss in epoch 90 is 61.40906524658203.\n",
      "Total loss in epoch 91 is 61.389400482177734.\n",
      "Total loss in epoch 92 is 61.57317352294922.\n",
      "Total loss in epoch 93 is 61.384857177734375.\n",
      "Total loss in epoch 94 is 61.414649963378906.\n",
      "Total loss in epoch 95 is 61.508323669433594.\n",
      "Total loss in epoch 96 is 61.59211349487305.\n",
      "Total loss in epoch 97 is 61.290565490722656.\n",
      "Total loss in epoch 98 is 61.355384826660156.\n",
      "Total loss in epoch 99 is 61.41252136230469.\n"
     ]
    }
   ],
   "source": [
    "#toy_tensor = X_tensor[:2]\n",
    "model = train(X_tensor, 30, 100, my_device) \n",
    "# You can add hyperparameters also here, change the number of epochs, batch size, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 458.00 MiB (GPU 1; 10.92 GiB total capacity; 4.20 GiB already allocated; 10.50 MiB free; 404.35 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-bc6fd21f20c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meverything\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib64/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    545\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 547\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    548\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-1061387edac2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0men2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0men3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0men3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mde1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mde1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mde1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 458.00 MiB (GPU 1; 10.92 GiB total capacity; 4.20 GiB already allocated; 10.50 MiB free; 404.35 MiB cached)"
     ]
    }
   ],
   "source": [
    "everything = model(X_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "everything.shape\n",
    "#X_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = everything[1511].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_true = X_tensor[1511].cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(sample_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = model.emb(X_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_tensor.shape)\n",
    "print(embs.shape)\n",
    "#embs=embs.flatten(start_dim=1)  # cheating here, figure out what to do with dimensions!!\n",
    "print(embs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = embs.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(3, random_state=700).fit(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated = TruncatedSVD(2).fit_transform(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (12,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(truncated[:,0], truncated[:,1], c=[{0:'b',1:'g',2:'r'}[x] for x in kmeans.labels_], \n",
    "            s=[{\"mouse\":10, \"skateboard\":50, \"airplane\":100}[x] for x in y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Informally analyze the performance of your model and clustering by examining cluster members to see the quality of the clusters and by experimenting with hyperparameters.  You can show your investigations here with markdown write-up.**\n",
    "\n",
    "This will be graded on good-faith effort. **5 points** absolute for a reasonable effort (think 2-3 paragraphs of discussion and **3 points** used to identify effort quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to use a chain of linear layers for my autoencoder since I saw it suggested in several articles and it's a relatively straighforward approach. The parameters I'm going to change are input size, batch size, and number of epochs. Changing the input size of course changes the representation of the data to the neural network. I'll try it once with an input size of 3, representing the pixel layer, and once with an input size of 300, representing rows of pixels. I look at random images and their reconstructions for my analysis, as well as the cluster above.\n",
    "\n",
    "For an input size of 300, batch size of 30, and 100 epochs, the reconstructed images include the correct shapes, and the overall color scheme is correct, but there are colored vertical lines running throughout the image. Since the clusters lie very close together, I'm struggling to see distinctive patterns there. Overall, the blue cluster tends to have many small points, and some medium ones, while the red one predominantly consists of medium ones, and the big dots cluster mainly in green. The grouping is not very clear though, many dots appear outside of these groups.\n",
    "\n",
    "Using the same input size and batch size, but a significantly lower number of epochs (20) unsurprisingly yields worse results. The images still broadly contain the same shapes, but the colors are completely off and there are even more of the stripes. In the cluster, the colors are distributed differently, and they are less clearly grouped. For example, the green cluster contains a semi-cluster of big points and directly next to it a semi-cluster of small points. The blue and red cluster seem to contain a mix of all three sizes. So 20 epochs obviously werent enough for the model to learn a good way of representing the images. \n",
    "\n",
    "In a third experiment, I used 100 epochs again and reduced the batch size to 5. The results with these hyperparameters are very similar to the ones with few epochs and larger batches, both in the images and in the cluster. \n",
    "\n",
    "When I change the input size to 3, with the same settings for batch size (30) and number of epochs (100), I get very interesting results. The images are reconstructed relatively accurately regarding the shapes of objects in them, but the colors are completely off (or yellow, to be more specific). This might also be caused by some error in my network though, maybe I accidentally switch the RBG values at some point. Looking at the cluster gives an even stranger sight. There are three clusters visible, but all the dots are big, so it seems like only pictures of airplanes are included in this clustering. Overall, it does not seem like the input size of 3 works well for this model and data (or I messed up the data when adjusting the layer sizes). \n",
    "\n",
    "I was surprised how good the results in the images were with the input size of 300, batch size 30, and 100 epochs since the model I chose is very simplistic. Even though the colourful striped are not ideal of course, the motives of the pictures are still recognizable. However, they don't seem to be to the clustering algorithm, which inarguably doesn't do very well with the encoded data of my model. \n",
    "\n",
    "Out of curiosity, I played around with the embedding size too in the end, since the default of 400 seemed very high to me. However, I found out that the image quality significantly decreases with smaller embedding sizes and the clustering gets even less clear, with all three clusters overlapping in the middle of the graph. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your analysis (bonus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Search for an apply a method to analyze cluster purity relative to ground truth (4 bonus points), and apply it to hyperparameter and model variants (3 bonus points).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch area for your convenience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will ignore anything after this line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 100, 100, 3])\n",
      "tensor([[[[ 0.3837,  0.3632,  0.2218],\n",
      "          [ 0.7108,  0.4557,  0.0433],\n",
      "          [ 0.7174,  0.4365,  0.0641],\n",
      "          ...,\n",
      "          [ 0.8126,  0.5518,  0.2043],\n",
      "          [ 0.7894,  0.5309,  0.1840],\n",
      "          [ 0.7771,  0.5312,  0.1488]],\n",
      "\n",
      "         [[ 0.3340,  0.3374,  0.2324],\n",
      "          [ 0.7019,  0.4503,  0.0547],\n",
      "          [ 0.6872,  0.4311,  0.0159],\n",
      "          ...,\n",
      "          [ 0.7742,  0.5522,  0.2168],\n",
      "          [ 0.7803,  0.5212,  0.1739],\n",
      "          [ 0.7681,  0.5105,  0.1657]],\n",
      "\n",
      "         [[ 0.3540,  0.3620,  0.2329],\n",
      "          [ 0.6599,  0.5088,  0.1207],\n",
      "          [ 0.7330,  0.4749,  0.0522],\n",
      "          ...,\n",
      "          [ 0.8089,  0.5470,  0.2202],\n",
      "          [ 0.7805,  0.5240,  0.1982],\n",
      "          [ 0.7745,  0.5145,  0.1744]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.2015,  0.2574,  0.2929],\n",
      "          [-0.6168, -0.7760, -0.8583],\n",
      "          [ 0.1837, -0.2710, -0.6108],\n",
      "          ...,\n",
      "          [ 0.1372,  0.0344, -0.1008],\n",
      "          [ 0.1844,  0.1562, -0.0168],\n",
      "          [ 0.5434,  0.4810,  0.3113]],\n",
      "\n",
      "         [[ 0.2948,  0.3722,  0.4859],\n",
      "          [ 0.0849, -0.4315, -0.6956],\n",
      "          [ 0.1372, -0.3693, -0.7165],\n",
      "          ...,\n",
      "          [-0.0559, -0.1158, -0.2355],\n",
      "          [ 0.1689,  0.2247,  0.0237],\n",
      "          [ 0.5619,  0.4594,  0.3008]],\n",
      "\n",
      "         [[-0.0674,  0.0038, -0.1215],\n",
      "          [ 0.0959, -0.3197, -0.6984],\n",
      "          [ 0.1345, -0.3679, -0.7131],\n",
      "          ...,\n",
      "          [-0.0223, -0.1006, -0.1806],\n",
      "          [ 0.1720,  0.2524,  0.0756],\n",
      "          [ 0.5531,  0.4931,  0.3201]]],\n",
      "\n",
      "\n",
      "        [[[ 1.8108,  1.7443,  1.5784],\n",
      "          [ 1.7892,  1.7243,  1.5584],\n",
      "          [ 1.7725,  1.7063,  1.5486],\n",
      "          ...,\n",
      "          [-0.1364,  0.0281, -0.0722],\n",
      "          [ 0.7633,  0.9328,  1.1170],\n",
      "          [ 0.3700,  0.8179,  1.0964]],\n",
      "\n",
      "         [[ 1.8089,  1.7338,  1.5692],\n",
      "          [ 1.8007,  1.7258,  1.5553],\n",
      "          [ 1.7823,  1.7174,  1.5520],\n",
      "          ...,\n",
      "          [ 0.8671,  1.0656,  1.2807],\n",
      "          [ 0.3710,  0.8486,  1.1124],\n",
      "          [ 1.5317,  1.7003,  1.5606]],\n",
      "\n",
      "         [[ 1.8039,  1.7375,  1.5708],\n",
      "          [ 1.8010,  1.7356,  1.5660],\n",
      "          [ 1.7986,  1.7325,  1.5600],\n",
      "          ...,\n",
      "          [ 0.5008,  0.6537,  1.0114],\n",
      "          [-0.6686, -0.4404, -0.1768],\n",
      "          [ 1.5746,  1.6314,  1.5606]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.6992,  0.9057,  0.9734],\n",
      "          [ 0.8137,  0.3397,  0.3112],\n",
      "          [ 1.1245,  1.3468,  1.3972],\n",
      "          ...,\n",
      "          [-0.7369, -0.9578, -0.9099],\n",
      "          [-0.8746, -0.9589, -0.8669],\n",
      "          [-0.7273, -0.6877, -0.6483]],\n",
      "\n",
      "         [[ 0.0673, -0.6537, -0.3351],\n",
      "          [-0.1157, -0.6641, -0.2328],\n",
      "          [ 0.0799, -0.6030, -0.4078],\n",
      "          ...,\n",
      "          [-0.6387, -0.9507, -0.8466],\n",
      "          [-0.8024, -0.9017, -0.8176],\n",
      "          [-0.7012, -0.6323, -0.6001]],\n",
      "\n",
      "         [[-0.0106, -0.6314, -0.6791],\n",
      "          [-0.2590, -0.4355, -0.3077],\n",
      "          [ 0.0205, -0.5851, -0.6712],\n",
      "          ...,\n",
      "          [-0.4740, -0.4294, -0.3877],\n",
      "          [-0.6011, -0.6300, -0.5275],\n",
      "          [-0.6292, -0.5642, -0.5339]]],\n",
      "\n",
      "\n",
      "        [[[ 1.4835,  1.4186,  1.2783],\n",
      "          [ 1.4384,  1.3748,  1.2363],\n",
      "          [ 1.3948,  1.3306,  1.2014],\n",
      "          ...,\n",
      "          [ 0.3908,  0.3292,  0.2764],\n",
      "          [ 0.8024,  0.7384,  0.6505],\n",
      "          [ 0.8165,  0.7527,  0.6646]],\n",
      "\n",
      "         [[ 1.4533,  1.3814,  1.2440],\n",
      "          [ 1.4565,  1.3839,  1.2407],\n",
      "          [ 0.8983,  0.8380,  0.7417],\n",
      "          ...,\n",
      "          [ 0.4558,  0.3942,  0.3377],\n",
      "          [ 0.6615,  0.5998,  0.5229],\n",
      "          [ 0.8339,  0.7720,  0.6828]],\n",
      "\n",
      "         [[ 1.4480,  1.3838,  1.2453],\n",
      "          [ 1.4826,  1.4190,  1.2745],\n",
      "          [ 1.5189,  1.4545,  1.3047],\n",
      "          ...,\n",
      "          [ 0.4204,  0.3603,  0.3054],\n",
      "          [ 0.8203,  0.7618,  0.6699],\n",
      "          [ 0.7614,  0.6963,  0.6166]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.3341, -1.2986, -1.1791],\n",
      "          [-0.3567, -0.3062, -0.2242],\n",
      "          [-1.0321, -0.9917, -0.8795],\n",
      "          ...,\n",
      "          [ 2.0144,  2.1210,  2.1170],\n",
      "          [ 2.0020,  2.0969,  2.1014],\n",
      "          [ 1.9552,  2.0344,  2.0385]],\n",
      "\n",
      "         [[-0.8573, -0.8127, -0.7108],\n",
      "          [-1.0614, -1.0129, -0.9060],\n",
      "          [-1.1099, -1.0704, -0.9550],\n",
      "          ...,\n",
      "          [ 2.0053,  2.1010,  2.1114],\n",
      "          [ 1.9849,  2.0783,  2.0924],\n",
      "          [ 1.9935,  2.0758,  2.0754]],\n",
      "\n",
      "         [[-0.8069, -0.7613, -0.6652],\n",
      "          [-0.7702, -0.7248, -0.6286],\n",
      "          [-0.5496, -0.4982, -0.4056],\n",
      "          ...,\n",
      "          [ 2.0529,  2.1439,  2.1520],\n",
      "          [ 2.0134,  2.1026,  2.1044],\n",
      "          [ 2.0450,  2.1362,  2.1381]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 0.7896,  0.5196,  0.1738],\n",
      "          [ 0.8017,  0.5075,  0.1507],\n",
      "          [ 0.7826,  0.4884,  0.1359],\n",
      "          ...,\n",
      "          [-0.5187, -0.8099, -0.9379],\n",
      "          [-0.5284, -0.7785, -0.9762],\n",
      "          [-0.4705, -0.7589, -0.9907]],\n",
      "\n",
      "         [[ 0.7686,  0.4809,  0.1963],\n",
      "          [ 0.7946,  0.4897,  0.1516],\n",
      "          [ 0.7796,  0.4836,  0.1248],\n",
      "          ...,\n",
      "          [-0.5658, -0.8302, -0.9802],\n",
      "          [-0.5399, -0.8017, -0.9571],\n",
      "          [-0.5222, -0.7840, -0.9405]],\n",
      "\n",
      "         [[ 0.7626,  0.4799,  0.1727],\n",
      "          [ 0.7661,  0.4956,  0.1207],\n",
      "          [ 0.7596,  0.4616,  0.1009],\n",
      "          ...,\n",
      "          [-0.5575, -0.8400, -0.9847],\n",
      "          [-0.5357, -0.7971, -0.9510],\n",
      "          [-0.5240, -0.7842, -0.9369]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 0.9835,  0.7761,  0.3762],\n",
      "          [ 1.0449,  0.8242,  0.5085],\n",
      "          [ 0.9942,  0.8321,  0.5205],\n",
      "          ...,\n",
      "          [-0.6652, -0.5930, -0.4217],\n",
      "          [-0.6742, -0.5969, -0.4349],\n",
      "          [-0.6284, -0.5737, -0.3330]],\n",
      "\n",
      "         [[ 1.0060,  0.8058,  0.4302],\n",
      "          [ 1.0449,  0.8768,  0.4965],\n",
      "          [ 0.9830,  0.8431,  0.4762],\n",
      "          ...,\n",
      "          [-0.7098, -0.5908, -0.4438],\n",
      "          [-0.7038, -0.5880, -0.4453],\n",
      "          [-0.6591, -0.5473, -0.4090]],\n",
      "\n",
      "         [[ 1.0559,  0.8267,  0.4919],\n",
      "          [ 1.0472,  0.8233,  0.4876],\n",
      "          [ 1.0467,  0.8338,  0.4192],\n",
      "          ...,\n",
      "          [-0.6152, -0.6438, -0.5395],\n",
      "          [-0.5589, -0.6157, -0.3356],\n",
      "          [-0.6855, -0.5642, -0.4099]]],\n",
      "\n",
      "\n",
      "        [[[-1.2922, -1.3567, -1.2068],\n",
      "          [-1.2902, -1.3566, -1.2093],\n",
      "          [-1.3275, -1.3904, -1.2169],\n",
      "          ...,\n",
      "          [-0.8613, -1.1765, -1.1182],\n",
      "          [-0.8154, -1.1415, -1.0958],\n",
      "          [-0.8382, -1.1629, -1.1106]],\n",
      "\n",
      "         [[-1.3120, -1.3722, -1.2247],\n",
      "          [-1.3502, -1.4170, -1.2643],\n",
      "          [-1.3447, -1.4064, -1.2299],\n",
      "          ...,\n",
      "          [-0.7250, -1.1461, -1.1132],\n",
      "          [-0.7775, -1.1553, -1.1375],\n",
      "          [-0.7856, -1.1501, -1.1089]],\n",
      "\n",
      "         [[-1.3200, -1.3804, -1.2254],\n",
      "          [-1.3437, -1.4040, -1.2516],\n",
      "          [-1.3715, -1.4314, -1.2489],\n",
      "          ...,\n",
      "          [-0.6646, -1.1067, -1.0821],\n",
      "          [-0.7085, -1.1274, -1.0961],\n",
      "          [-0.7076, -1.0829, -1.0683]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.2914, -1.2121, -1.1930],\n",
      "          [-1.3249, -1.2457, -1.2247],\n",
      "          [-1.3216, -1.2417, -1.2189],\n",
      "          ...,\n",
      "          [-1.2528, -1.1913, -1.1610],\n",
      "          [-1.2467, -1.1617, -1.1874],\n",
      "          [-1.2496, -1.1580, -1.1556]],\n",
      "\n",
      "         [[-1.2697, -1.1884, -1.1978],\n",
      "          [-1.3049, -1.2164, -1.2286],\n",
      "          [-1.2963, -1.2165, -1.1935],\n",
      "          ...,\n",
      "          [-1.2926, -1.2242, -1.1799],\n",
      "          [-1.2669, -1.1726, -1.1486],\n",
      "          [-1.2485, -1.1569, -1.1324]],\n",
      "\n",
      "         [[-1.2477, -1.1944, -1.1670],\n",
      "          [-1.2672, -1.2168, -1.2147],\n",
      "          [-1.2622, -1.2076, -1.1745],\n",
      "          ...,\n",
      "          [-1.2504, -1.1728, -1.1192],\n",
      "          [-1.2899, -1.2135, -1.1581],\n",
      "          [-1.2626, -1.2072, -1.1536]]],\n",
      "\n",
      "\n",
      "        [[[-1.5278, -1.5782, -1.4829],\n",
      "          [-1.5240, -1.5767, -1.4837],\n",
      "          [-1.5359, -1.5847, -1.4923],\n",
      "          ...,\n",
      "          [-1.2568, -1.5431, -1.4669],\n",
      "          [-1.2330, -1.5564, -1.4786],\n",
      "          [-1.3241, -1.5669, -1.4825]],\n",
      "\n",
      "         [[-1.5490, -1.5941, -1.5017],\n",
      "          [-1.5752, -1.6274, -1.5305],\n",
      "          [-1.5821, -1.6295, -1.5323],\n",
      "          ...,\n",
      "          [-1.3221, -1.6069, -1.5243],\n",
      "          [-1.4244, -1.6007, -1.5105],\n",
      "          [-1.4834, -1.5815, -1.4937]],\n",
      "\n",
      "         [[-1.4782, -1.6031, -1.5026],\n",
      "          [-1.5826, -1.6151, -1.5309],\n",
      "          [-1.5980, -1.6432, -1.5407],\n",
      "          ...,\n",
      "          [-1.5755, -1.6535, -1.4959],\n",
      "          [-1.5727, -1.6294, -1.5195],\n",
      "          [-1.4421, -1.5504, -1.4866]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.1208, -1.3994, -1.2763],\n",
      "          [-1.1226, -1.3925, -1.2811],\n",
      "          [-1.2492, -1.4182, -1.2896],\n",
      "          ...,\n",
      "          [-1.0235, -1.3810, -1.2446],\n",
      "          [-1.0749, -1.3645, -1.2571],\n",
      "          [-1.0520, -1.3718, -1.2378]],\n",
      "\n",
      "         [[-1.1417, -1.3618, -1.2674],\n",
      "          [-1.1617, -1.3763, -1.2847],\n",
      "          [-1.0812, -1.3333, -1.2216],\n",
      "          ...,\n",
      "          [-1.1078, -1.3538, -1.2354],\n",
      "          [-1.1262, -1.3580, -1.2452],\n",
      "          [-1.0661, -1.3129, -1.2007]],\n",
      "\n",
      "         [[-1.1481, -1.3676, -1.2507],\n",
      "          [-1.1394, -1.3614, -1.2426],\n",
      "          [-1.1339, -1.3813, -1.2444],\n",
      "          ...,\n",
      "          [-1.1092, -1.3586, -1.2434],\n",
      "          [-1.1071, -1.3131, -1.1855],\n",
      "          [-1.0937, -1.3358, -1.1950]]]], device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "b = Batcher(X_tensor, device=my_device, batch_size=10, max_iter=3)\n",
    "for split in b:\n",
    "    for batch in split:\n",
    "        print(batch.shape)\n",
    "        print(batch)\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# solve problem with CUDA and dimensions.. height*width*3 makes more sense!\n",
    "# try to add different/other layers to the model, try different layer sizes..\n",
    "# analysis part.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
